<h1>Product Evaluation</h1>

<p>Our approach to the evaluation of our product closely followed the processes and methods that some of our team learned to employ in this year's 'Human Computer Interaction Module'. From this we learned that both a qualitative and a Quantitative approach were necessary for a good evaluation protocol, followed by a heuristic self evaluation. Specifics of these procedures are detailed in this document. As a result, our evaluation of the product was mostly focused on the user and admin experience of the system, as opposed to a technical evaluation.</p>

<h4>Quantitative study protocol</h4>
<p>Our aim for the quantitative study protocol was to gain measurable opinions on some of the features and styles of the system, and to differentiate between juxtaposing points so development decisions could be made. We achieved this by distributing a Likert scale questionnaire via the Survey Monkey service. We chose this approach as it was easy to distribute, allowed for a large number of results and was easy to analyze after the results were collected. A copy of the survey is attached in this directory. After the results were collected we analyzed the results with the Survey Monkey built in tools. The results of are also attached. The target group of this procedure was general UOB students in order to gain user insights. We did not feel that the admin's needed to be studied quantitatively as a qualitative survey would be more appropriate for them.</p>


<h4>Qualitative study protocol</h4>
<p>Our qualitative study protocol was three pronged. On the user/student side, we conducted both interviews while the user had access to the system, as well as focus groups where the group had access to the system. The student interviews were conducted on students in the Queen's building as these were most likely to be using the finished system. The aim of the student interviews was to gain subjective views of our product, as well as constructive criticism and fresh insights. It also allowed for further bug detection. We also used user/student focus groups where we facilitated discussions about the system and recorded these opinions. For the admins and client we conducted interviews with staff whilst giving them access to the system to gauge the usability and detect and flaws from the admin perspective. All of these studies were conducted with a facilitator leading the study, an observer taking notes, and the test subject(s). The results of these tests are reflected in our iterative refinements section, however a sample interview script is attached.</p>

<h4>Heuristic evaluation</h4>
<p>Using the results from all the previous evaluations we employed two heuristic techniques. Firstly, we used the Nielson Heuristics framework to analyze our product and realise improvements. A copy of this can be found in this directory. Secondly, we gave Nielson severity ratings to  the issues in our system to help identify the critical issues that needed addressing. Using these two techniques, combined with the user feedback from the other evaluation methods we employed, we were able to decide on what aspects of the product to adapt in the tail end of the project.</p>

<h4>Iterative refinements</h4>
<p>TODO</p>
